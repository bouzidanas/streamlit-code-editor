{"ast":null,"code":"// Licensed to the Apache Software Foundation (ASF) under one\n// or more contributor license agreements.  See the NOTICE file\n// distributed with this work for additional information\n// regarding copyright ownership.  The ASF licenses this file\n// to you under the Apache License, Version 2.0 (the\n// \"License\"); you may not use this file except in compliance\n// with the License.  You may obtain a copy of the License at\n//\n//   http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing,\n// software distributed under the License is distributed on an\n// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n// KIND, either express or implied.  See the License for the\n// specific language governing permissions and limitations\n// under the License.\nimport { Column } from './column';\nimport { Schema } from './schema';\nimport { RecordBatch, _InternalEmptyPlaceholderRecordBatch } from './recordbatch';\nimport { RecordBatchReader } from './ipc/reader';\nimport { Struct } from './type';\nimport { selectColumnArgs, selectArgs } from './util/args';\nimport { isPromise, isIterable, isAsyncIterable } from './util/compat';\nimport { RecordBatchFileWriter, RecordBatchStreamWriter } from './ipc/writer';\nimport { distributeColumnsIntoRecordBatches, distributeVectorsIntoRecordBatches } from './util/recordbatch';\nimport { Chunked, StructVector } from './vector/index';\nexport class Table extends Chunked {\n  constructor() {\n    let schema = null;\n    for (var _len = arguments.length, args = new Array(_len), _key = 0; _key < _len; _key++) {\n      args[_key] = arguments[_key];\n    }\n    if (args[0] instanceof Schema) {\n      schema = args.shift();\n    }\n    let chunks = selectArgs(RecordBatch, args);\n    if (!schema && !(schema = chunks[0] && chunks[0].schema)) {\n      throw new TypeError('Table must be initialized with a Schema or at least one RecordBatch');\n    }\n    chunks[0] || (chunks[0] = new _InternalEmptyPlaceholderRecordBatch(schema));\n    super(new Struct(schema.fields), chunks);\n    this._schema = schema;\n    this._chunks = chunks;\n  }\n  /** @nocollapse */\n  static empty() {\n    let schema = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : new Schema([]);\n    return new Table(schema, []);\n  }\n  /** @nocollapse */\n  static from(input) {\n    if (!input) {\n      return Table.empty();\n    }\n    if (typeof input === 'object') {\n      let table = isIterable(input['values']) ? tableFromIterable(input) : isAsyncIterable(input['values']) ? tableFromAsyncIterable(input) : null;\n      if (table !== null) {\n        return table;\n      }\n    }\n    let reader = RecordBatchReader.from(input);\n    if (isPromise(reader)) {\n      return (async () => await Table.from(await reader))();\n    }\n    if (reader.isSync() && (reader = reader.open())) {\n      return !reader.schema ? Table.empty() : new Table(reader.schema, [...reader]);\n    }\n    return (async opening => {\n      const reader = await opening;\n      const schema = reader.schema;\n      const batches = [];\n      if (schema) {\n        for await (let batch of reader) {\n          batches.push(batch);\n        }\n        return new Table(schema, batches);\n      }\n      return Table.empty();\n    })(reader.open());\n  }\n  /** @nocollapse */\n  static async fromAsync(source) {\n    return await Table.from(source);\n  }\n  /** @nocollapse */\n  static fromStruct(vector) {\n    return Table.new(vector.data.childData, vector.type.children);\n  }\n  /** @nocollapse */\n  static new() {\n    for (var _len2 = arguments.length, cols = new Array(_len2), _key2 = 0; _key2 < _len2; _key2++) {\n      cols[_key2] = arguments[_key2];\n    }\n    return new Table(...distributeColumnsIntoRecordBatches(selectColumnArgs(cols)));\n  }\n  get schema() {\n    return this._schema;\n  }\n  get length() {\n    return this._length;\n  }\n  get chunks() {\n    return this._chunks;\n  }\n  get numCols() {\n    return this._numChildren;\n  }\n  clone() {\n    let chunks = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : this._chunks;\n    return new Table(this._schema, chunks);\n  }\n  getColumn(name) {\n    return this.getColumnAt(this.getColumnIndex(name));\n  }\n  getColumnAt(index) {\n    return this.getChildAt(index);\n  }\n  getColumnIndex(name) {\n    return this._schema.fields.findIndex(f => f.name === name);\n  }\n  getChildAt(index) {\n    if (index < 0 || index >= this.numChildren) {\n      return null;\n    }\n    let field, child;\n    const fields = this._schema.fields;\n    const columns = this._children || (this._children = []);\n    if (child = columns[index]) {\n      return child;\n    }\n    if (field = fields[index]) {\n      const chunks = this._chunks.map(chunk => chunk.getChildAt(index)).filter(vec => vec != null);\n      if (chunks.length > 0) {\n        return columns[index] = new Column(field, chunks);\n      }\n    }\n    return null;\n  }\n  // @ts-ignore\n  serialize() {\n    let encoding = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : 'binary';\n    let stream = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : true;\n    const Writer = !stream ? RecordBatchFileWriter : RecordBatchStreamWriter;\n    return Writer.writeAll(this).toUint8Array(true);\n  }\n  count() {\n    return this._length;\n  }\n  select() {\n    const nameToIndex = this._schema.fields.reduce((m, f, i) => m.set(f.name, i), new Map());\n    for (var _len3 = arguments.length, columnNames = new Array(_len3), _key3 = 0; _key3 < _len3; _key3++) {\n      columnNames[_key3] = arguments[_key3];\n    }\n    return this.selectAt(...columnNames.map(columnName => nameToIndex.get(columnName)).filter(x => x > -1));\n  }\n  selectAt() {\n    for (var _len4 = arguments.length, columnIndices = new Array(_len4), _key4 = 0; _key4 < _len4; _key4++) {\n      columnIndices[_key4] = arguments[_key4];\n    }\n    const schema = this._schema.selectAt(...columnIndices);\n    return new Table(schema, this._chunks.map(_ref => {\n      let {\n        length,\n        data: {\n          childData\n        }\n      } = _ref;\n      return new RecordBatch(schema, length, columnIndices.map(i => childData[i]).filter(Boolean));\n    }));\n  }\n  assign(other) {\n    var _this = this;\n    const fields = this._schema.fields;\n    const [indices, oldToNew] = other.schema.fields.reduce((memo, f2, newIdx) => {\n      const [indices, oldToNew] = memo;\n      const i = fields.findIndex(f => f.name === f2.name);\n      ~i ? oldToNew[i] = newIdx : indices.push(newIdx);\n      return memo;\n    }, [[], []]);\n    const schema = this._schema.assign(other.schema);\n    const columns = [...fields.map(function (_f, i, _fs) {\n      let j = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : oldToNew[i];\n      return j === undefined ? _this.getColumnAt(i) : other.getColumnAt(j);\n    }), ...indices.map(i => other.getColumnAt(i))].filter(Boolean);\n    return new Table(...distributeVectorsIntoRecordBatches(schema, columns));\n  }\n}\nfunction tableFromIterable(input) {\n  const {\n    type\n  } = input;\n  if (type instanceof Struct) {\n    return Table.fromStruct(StructVector.from(input));\n  }\n  return null;\n}\nfunction tableFromAsyncIterable(input) {\n  const {\n    type\n  } = input;\n  if (type instanceof Struct) {\n    return StructVector.from(input).then(vector => Table.fromStruct(vector));\n  }\n  return null;\n}","map":{"version":3,"names":["Column","Schema","RecordBatch","_InternalEmptyPlaceholderRecordBatch","RecordBatchReader","Struct","selectColumnArgs","selectArgs","isPromise","isIterable","isAsyncIterable","RecordBatchFileWriter","RecordBatchStreamWriter","distributeColumnsIntoRecordBatches","distributeVectorsIntoRecordBatches","Chunked","StructVector","Table","constructor","schema","_len","arguments","length","args","Array","_key","shift","chunks","TypeError","fields","_schema","_chunks","empty","undefined","from","input","table","tableFromIterable","tableFromAsyncIterable","reader","isSync","open","opening","batches","batch","push","fromAsync","source","fromStruct","vector","new","data","childData","type","children","_len2","cols","_key2","_length","numCols","_numChildren","clone","getColumn","name","getColumnAt","getColumnIndex","index","getChildAt","findIndex","f","numChildren","field","child","columns","_children","map","chunk","filter","vec","serialize","encoding","stream","Writer","writeAll","toUint8Array","count","select","nameToIndex","reduce","m","i","set","Map","_len3","columnNames","_key3","selectAt","columnName","get","x","_len4","columnIndices","_key4","_ref","Boolean","assign","other","_this","indices","oldToNew","memo","f2","newIdx","_f","_fs","j","then"],"sources":["table.ts"],"sourcesContent":["// Licensed to the Apache Software Foundation (ASF) under one\n// or more contributor license agreements.  See the NOTICE file\n// distributed with this work for additional information\n// regarding copyright ownership.  The ASF licenses this file\n// to you under the Apache License, Version 2.0 (the\n// \"License\"); you may not use this file except in compliance\n// with the License.  You may obtain a copy of the License at\n//\n//   http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing,\n// software distributed under the License is distributed on an\n// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n// KIND, either express or implied.  See the License for the\n// specific language governing permissions and limitations\n// under the License.\n\nimport { Data } from './data';\nimport { Column } from './column';\nimport { Schema, Field } from './schema';\nimport { RecordBatch, _InternalEmptyPlaceholderRecordBatch } from './recordbatch';\nimport { DataFrame } from './compute/dataframe';\nimport { RecordBatchReader } from './ipc/reader';\nimport { DataType, RowLike, Struct } from './type';\nimport { selectColumnArgs, selectArgs } from './util/args';\nimport { Clonable, Sliceable, Applicative } from './vector';\nimport { isPromise, isIterable, isAsyncIterable } from './util/compat';\nimport { RecordBatchFileWriter, RecordBatchStreamWriter } from './ipc/writer';\nimport { distributeColumnsIntoRecordBatches, distributeVectorsIntoRecordBatches } from './util/recordbatch';\nimport { Vector, Chunked, StructVector, VectorBuilderOptions, VectorBuilderOptionsAsync } from './vector/index';\n\ntype VectorMap = { [key: string]: Vector };\ntype Fields<T extends { [key: string]: DataType }> = (keyof T)[] | Field<T[keyof T]>[];\ntype ChildData<T extends { [key: string]: DataType }> = Data<T[keyof T]>[] | Vector<T[keyof T]>[];\ntype Columns<T extends { [key: string]: DataType }> = Column<T[keyof T]>[] | Column<T[keyof T]>[][];\n\nexport interface Table<T extends { [key: string]: DataType } = any> {\n\n    get(index: number): Struct<T>['TValue'];\n    [Symbol.iterator](): IterableIterator<RowLike<T>>;\n\n    slice(begin?: number, end?: number): Table<T>;\n    concat(...others: Vector<Struct<T>>[]): Table<T>;\n    clone(chunks?: RecordBatch<T>[], offsets?: Uint32Array): Table<T>;\n\n    scan(next: import('./compute/dataframe').NextFunc, bind?: import('./compute/dataframe').BindFunc): void;\n    scanReverse(next: import('./compute/dataframe').NextFunc, bind?: import('./compute/dataframe').BindFunc): void;\n    countBy(name: import('./compute/predicate').Col | string): import('./compute/dataframe').CountByResult;\n    filter(predicate: import('./compute/predicate').Predicate): import('./compute/dataframe').FilteredDataFrame<T>;\n}\n\nexport class Table<T extends { [key: string]: DataType } = any>\n    extends Chunked<Struct<T>>\n    implements DataFrame<T>,\n               Clonable<Table<T>>,\n               Sliceable<Table<T>>,\n               Applicative<Struct<T>, Table<T>> {\n\n    /** @nocollapse */\n    public static empty<T extends { [key: string]: DataType } = {}>(schema = new Schema<T>([])) { return new Table<T>(schema, []); }\n\n    public static from(): Table<{}>;\n    public static from<T extends { [key: string]: DataType } = any>(source: RecordBatchReader<T>): Table<T>;\n    public static from<T extends { [key: string]: DataType } = any>(source: import('./ipc/reader').FromArg0): Table<T>;\n    public static from<T extends { [key: string]: DataType } = any>(source: import('./ipc/reader').FromArg2): Table<T>;\n    public static from<T extends { [key: string]: DataType } = any>(source: import('./ipc/reader').FromArg1): Promise<Table<T>>;\n    public static from<T extends { [key: string]: DataType } = any>(source: import('./ipc/reader').FromArg3): Promise<Table<T>>;\n    public static from<T extends { [key: string]: DataType } = any>(source: import('./ipc/reader').FromArg4): Promise<Table<T>>;\n    public static from<T extends { [key: string]: DataType } = any>(source: import('./ipc/reader').FromArg5): Promise<Table<T>>;\n    public static from<T extends { [key: string]: DataType } = any>(source: PromiseLike<RecordBatchReader<T>>): Promise<Table<T>>;\n    public static from<T extends { [key: string]: DataType } = any, TNull = any>(options: VectorBuilderOptions<Struct<T>, TNull>): Table<T>;\n    public static from<T extends { [key: string]: DataType } = any, TNull = any>(options: VectorBuilderOptionsAsync<Struct<T>, TNull>): Promise<Table<T>>;\n    /** @nocollapse */\n    public static from<T extends { [key: string]: DataType } = any, TNull = any>(input?: any) {\n\n        if (!input) { return Table.empty(); }\n\n        if (typeof input === 'object') {\n            let table = isIterable(input['values']) ? tableFromIterable<T, TNull>(input)\n                 : isAsyncIterable(input['values']) ? tableFromAsyncIterable<T, TNull>(input)\n                                                    : null;\n            if (table !== null) { return table; }\n        }\n\n        let reader = RecordBatchReader.from<T>(input) as RecordBatchReader<T> | Promise<RecordBatchReader<T>>;\n\n        if (isPromise<RecordBatchReader<T>>(reader)) {\n            return (async () => await Table.from(await reader))();\n        }\n        if (reader.isSync() && (reader = reader.open())) {\n            return !reader.schema ? Table.empty() : new Table<T>(reader.schema, [...reader]);\n        }\n        return (async (opening) => {\n            const reader = await opening;\n            const schema = reader.schema;\n            const batches: RecordBatch[] = [];\n            if (schema) {\n                for await (let batch of reader) {\n                    batches.push(batch);\n                }\n                return new Table<T>(schema, batches);\n            }\n            return Table.empty();\n        })(reader.open());\n    }\n\n    /** @nocollapse */\n    public static async fromAsync<T extends { [key: string]: DataType } = any>(source: import('./ipc/reader').FromArgs): Promise<Table<T>> {\n        return await Table.from<T>(source as any);\n    }\n\n    /** @nocollapse */\n    public static fromStruct<T extends { [key: string]: DataType } = any>(vector: Vector<Struct<T>>) {\n        return Table.new<T>(vector.data.childData as Data<T[keyof T]>[], vector.type.children);\n    }\n\n    /**\n     * @summary Create a new Table from a collection of Columns or Vectors,\n     * with an optional list of names or Fields.\n     *\n     *\n     * `Table.new` accepts an Object of\n     * Columns or Vectors, where the keys will be used as the field names\n     * for the Schema:\n     * ```ts\n     * const i32s = Int32Vector.from([1, 2, 3]);\n     * const f32s = Float32Vector.from([.1, .2, .3]);\n     * const table = Table.new({ i32: i32s, f32: f32s });\n     * assert(table.schema.fields[0].name === 'i32');\n     * ```\n     *\n     * It also accepts a a list of Vectors with an optional list of names or\n     * Fields for the resulting Schema. If the list is omitted or a name is\n     * missing, the numeric index of each Vector will be used as the name:\n     * ```ts\n     * const i32s = Int32Vector.from([1, 2, 3]);\n     * const f32s = Float32Vector.from([.1, .2, .3]);\n     * const table = Table.new([i32s, f32s], ['i32']);\n     * assert(table.schema.fields[0].name === 'i32');\n     * assert(table.schema.fields[1].name === '1');\n     * ```\n     *\n     * If the supplied arguments are Columns, `Table.new` will infer the Schema\n     * from the Columns:\n     * ```ts\n     * const i32s = Column.new('i32', Int32Vector.from([1, 2, 3]));\n     * const f32s = Column.new('f32', Float32Vector.from([.1, .2, .3]));\n     * const table = Table.new(i32s, f32s);\n     * assert(table.schema.fields[0].name === 'i32');\n     * assert(table.schema.fields[1].name === 'f32');\n     * ```\n     *\n     * If the supplied Vector or Column lengths are unequal, `Table.new` will\n     * extend the lengths of the shorter Columns, allocating additional bytes\n     * to represent the additional null slots. The memory required to allocate\n     * these additional bitmaps can be computed as:\n     * ```ts\n     * let additionalBytes = 0;\n     * for (let vec in shorter_vectors) {\n     *     additionalBytes += (((longestLength - vec.length) + 63) & ~63) >> 3;\n     * }\n     * ```\n     *\n     * For example, an additional null bitmap for one million null values would require\n     * 125,000 bytes (`((1e6 + 63) & ~63) >> 3`), or approx. `0.11MiB`\n     */\n    public static new<T extends { [key: string]: DataType } = any>(...columns: Columns<T>): Table<T>;\n    public static new<T extends VectorMap = any>(children: T): Table<{ [P in keyof T]: T[P]['type'] }>;\n    public static new<T extends { [key: string]: DataType } = any>(children: ChildData<T>, fields?: Fields<T>): Table<T>;\n    /** @nocollapse */\n    public static new(...cols: any[]) {\n        return new Table(...distributeColumnsIntoRecordBatches(selectColumnArgs(cols)));\n    }\n\n    constructor(batches: RecordBatch<T>[]);\n    constructor(...batches: RecordBatch<T>[]);\n    constructor(schema: Schema<T>, batches: RecordBatch<T>[]);\n    constructor(schema: Schema<T>, ...batches: RecordBatch<T>[]);\n    constructor(...args: any[]) {\n\n        let schema: Schema<T> = null!;\n\n        if (args[0] instanceof Schema) { schema = args.shift(); }\n\n        let chunks = selectArgs<RecordBatch<T>>(RecordBatch, args);\n\n        if (!schema && !(schema = chunks[0] && chunks[0].schema)) {\n            throw new TypeError('Table must be initialized with a Schema or at least one RecordBatch');\n        }\n\n        chunks[0] || (chunks[0] = new _InternalEmptyPlaceholderRecordBatch(schema));\n\n        super(new Struct(schema.fields), chunks);\n\n        this._schema = schema;\n        this._chunks = chunks;\n    }\n\n    protected _schema: Schema<T>;\n    // List of inner RecordBatches\n    protected _chunks: RecordBatch<T>[];\n    protected _children?: Column<T[keyof T]>[];\n\n    public get schema() { return this._schema; }\n    public get length() { return this._length; }\n    public get chunks() { return this._chunks; }\n    public get numCols() { return this._numChildren; }\n\n    public clone(chunks = this._chunks) {\n        return new Table<T>(this._schema, chunks);\n    }\n\n    public getColumn<R extends keyof T>(name: R): Column<T[R]> {\n        return this.getColumnAt(this.getColumnIndex(name)) as Column<T[R]>;\n    }\n    public getColumnAt<R extends DataType = any>(index: number): Column<R> | null {\n        return this.getChildAt(index);\n    }\n    public getColumnIndex<R extends keyof T>(name: R) {\n        return this._schema.fields.findIndex((f) => f.name === name);\n    }\n    public getChildAt<R extends DataType = any>(index: number): Column<R> | null {\n        if (index < 0 || index >= this.numChildren) { return null; }\n        let field: Field<R>, child: Column<R>;\n        const fields = (this._schema as Schema<any>).fields;\n        const columns = this._children || (this._children = []) as Column[];\n        if (child = columns[index]) { return child as Column<R>; }\n        if (field = fields[index]) {\n            const chunks = this._chunks\n                .map((chunk) => chunk.getChildAt<R>(index))\n                .filter((vec): vec is Vector<R> => vec != null);\n            if (chunks.length > 0) {\n                return (columns[index] = new Column<R>(field, chunks));\n            }\n        }\n        return null;\n    }\n\n    // @ts-ignore\n    public serialize(encoding = 'binary', stream = true) {\n        const Writer = !stream\n            ? RecordBatchFileWriter\n            : RecordBatchStreamWriter;\n        return Writer.writeAll(this).toUint8Array(true);\n    }\n    public count(): number {\n        return this._length;\n    }\n    public select<K extends keyof T = any>(...columnNames: K[]) {\n        const nameToIndex = this._schema.fields.reduce((m, f, i) => m.set(f.name as K, i), new Map<K, number>());\n        return this.selectAt(...columnNames.map((columnName) => nameToIndex.get(columnName)!).filter((x) => x > -1));\n    }\n    public selectAt<K extends T[keyof T] = any>(...columnIndices: number[]) {\n        const schema = this._schema.selectAt<K>(...columnIndices);\n        return new Table(schema, this._chunks.map(({ length, data: { childData } }) => {\n            return new RecordBatch(schema, length, columnIndices.map((i) => childData[i]).filter(Boolean));\n        }));\n    }\n    public assign<R extends { [key: string]: DataType } = any>(other: Table<R>) {\n\n        const fields = this._schema.fields;\n        const [indices, oldToNew] = other.schema.fields.reduce((memo, f2, newIdx) => {\n            const [indices, oldToNew] = memo;\n            const i = fields.findIndex((f) => f.name === f2.name);\n            ~i ? (oldToNew[i] = newIdx) : indices.push(newIdx);\n            return memo;\n        }, [[], []] as number[][]);\n\n        const schema = this._schema.assign(other.schema);\n        const columns = [\n            ...fields.map((_f, i, _fs, j = oldToNew[i]) =>\n                (j === undefined ? this.getColumnAt(i) : other.getColumnAt(j))!),\n            ...indices.map((i) => other.getColumnAt(i)!)\n        ].filter(Boolean) as Column<(T & R)[keyof T | keyof R]>[];\n\n        return new Table<T & R>(...distributeVectorsIntoRecordBatches<any>(schema, columns));\n    }\n}\n\nfunction tableFromIterable<T extends { [key: string]: DataType } = any, TNull = any>(input: VectorBuilderOptions<Struct<T>, TNull>) {\n    const { type } = input;\n    if (type instanceof Struct) {\n        return Table.fromStruct(StructVector.from(input as VectorBuilderOptions<Struct<T>, TNull>));\n    }\n    return null;\n}\n\nfunction tableFromAsyncIterable<T extends { [key: string]: DataType } = any, TNull = any>(input: VectorBuilderOptionsAsync<Struct<T>, TNull>) {\n    const { type } = input;\n    if (type instanceof Struct) {\n        return StructVector.from(input as VectorBuilderOptionsAsync<Struct<T>, TNull>).then((vector) => Table.fromStruct(vector));\n    }\n    return null;\n}\n"],"mappings":"AAAA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAGA,SAASA,MAAM,QAAQ,UAAU;AACjC,SAASC,MAAM,QAAe,UAAU;AACxC,SAASC,WAAW,EAAEC,oCAAoC,QAAQ,eAAe;AAEjF,SAASC,iBAAiB,QAAQ,cAAc;AAChD,SAA4BC,MAAM,QAAQ,QAAQ;AAClD,SAASC,gBAAgB,EAAEC,UAAU,QAAQ,aAAa;AAE1D,SAASC,SAAS,EAAEC,UAAU,EAAEC,eAAe,QAAQ,eAAe;AACtE,SAASC,qBAAqB,EAAEC,uBAAuB,QAAQ,cAAc;AAC7E,SAASC,kCAAkC,EAAEC,kCAAkC,QAAQ,oBAAoB;AAC3G,SAAiBC,OAAO,EAAEC,YAAY,QAAyD,gBAAgB;AAsB/G,OAAM,MAAOC,KACT,SAAQF,OAAkB;EA8H1BG,YAAA,EAA0B;IAEtB,IAAIC,MAAM,GAAc,IAAK;IAAC,SAAAC,IAAA,GAAAC,SAAA,CAAAC,MAAA,EAFnBC,IAAW,OAAAC,KAAA,CAAAJ,IAAA,GAAAK,IAAA,MAAAA,IAAA,GAAAL,IAAA,EAAAK,IAAA;MAAXF,IAAW,CAAAE,IAAA,IAAAJ,SAAA,CAAAI,IAAA;IAAA;IAItB,IAAIF,IAAI,CAAC,CAAC,CAAC,YAAYtB,MAAM,EAAE;MAAEkB,MAAM,GAAGI,IAAI,CAACG,KAAK,EAAE;;IAEtD,IAAIC,MAAM,GAAGpB,UAAU,CAAiBL,WAAW,EAAEqB,IAAI,CAAC;IAE1D,IAAI,CAACJ,MAAM,IAAI,EAAEA,MAAM,GAAGQ,MAAM,CAAC,CAAC,CAAC,IAAIA,MAAM,CAAC,CAAC,CAAC,CAACR,MAAM,CAAC,EAAE;MACtD,MAAM,IAAIS,SAAS,CAAC,qEAAqE,CAAC;;IAG9FD,MAAM,CAAC,CAAC,CAAC,KAAKA,MAAM,CAAC,CAAC,CAAC,GAAG,IAAIxB,oCAAoC,CAACgB,MAAM,CAAC,CAAC;IAE3E,KAAK,CAAC,IAAId,MAAM,CAACc,MAAM,CAACU,MAAM,CAAC,EAAEF,MAAM,CAAC;IAExC,IAAI,CAACG,OAAO,GAAGX,MAAM;IACrB,IAAI,CAACY,OAAO,GAAGJ,MAAM;EACzB;EA1IA;EACO,OAAOK,KAAKA,CAAA,EAAuE;IAAA,IAA1Bb,MAAA,GAAAE,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAY,SAAA,GAAAZ,SAAA,MAAS,IAAIpB,MAAM,CAAI,EAAE,CAAC;IAAI,OAAO,IAAIgB,KAAK,CAAIE,MAAM,EAAE,EAAE,CAAC;EAAE;EAa/H;EACO,OAAOe,IAAIA,CAA2DC,KAAW;IAEpF,IAAI,CAACA,KAAK,EAAE;MAAE,OAAOlB,KAAK,CAACe,KAAK,EAAE;;IAElC,IAAI,OAAOG,KAAK,KAAK,QAAQ,EAAE;MAC3B,IAAIC,KAAK,GAAG3B,UAAU,CAAC0B,KAAK,CAAC,QAAQ,CAAC,CAAC,GAAGE,iBAAiB,CAAWF,KAAK,CAAC,GACrEzB,eAAe,CAACyB,KAAK,CAAC,QAAQ,CAAC,CAAC,GAAGG,sBAAsB,CAAWH,KAAK,CAAC,GACvC,IAAI;MAC9C,IAAIC,KAAK,KAAK,IAAI,EAAE;QAAE,OAAOA,KAAK;;;IAGtC,IAAIG,MAAM,GAAGnC,iBAAiB,CAAC8B,IAAI,CAAIC,KAAK,CAAyD;IAErG,IAAI3B,SAAS,CAAuB+B,MAAM,CAAC,EAAE;MACzC,OAAO,CAAC,YAAY,MAAMtB,KAAK,CAACiB,IAAI,CAAC,MAAMK,MAAM,CAAC,EAAC,CAAE;;IAEzD,IAAIA,MAAM,CAACC,MAAM,EAAE,KAAKD,MAAM,GAAGA,MAAM,CAACE,IAAI,EAAE,CAAC,EAAE;MAC7C,OAAO,CAACF,MAAM,CAACpB,MAAM,GAAGF,KAAK,CAACe,KAAK,EAAE,GAAG,IAAIf,KAAK,CAAIsB,MAAM,CAACpB,MAAM,EAAE,CAAC,GAAGoB,MAAM,CAAC,CAAC;;IAEpF,OAAO,CAAC,MAAOG,OAAO,IAAI;MACtB,MAAMH,MAAM,GAAG,MAAMG,OAAO;MAC5B,MAAMvB,MAAM,GAAGoB,MAAM,CAACpB,MAAM;MAC5B,MAAMwB,OAAO,GAAkB,EAAE;MACjC,IAAIxB,MAAM,EAAE;QACR,WAAW,IAAIyB,KAAK,IAAIL,MAAM,EAAE;UAC5BI,OAAO,CAACE,IAAI,CAACD,KAAK,CAAC;;QAEvB,OAAO,IAAI3B,KAAK,CAAIE,MAAM,EAAEwB,OAAO,CAAC;;MAExC,OAAO1B,KAAK,CAACe,KAAK,EAAE;IACxB,CAAC,EAAEO,MAAM,CAACE,IAAI,EAAE,CAAC;EACrB;EAEA;EACO,aAAaK,SAASA,CAA8CC,MAAuC;IAC9G,OAAO,MAAM9B,KAAK,CAACiB,IAAI,CAAIa,MAAa,CAAC;EAC7C;EAEA;EACO,OAAOC,UAAUA,CAA8CC,MAAyB;IAC3F,OAAOhC,KAAK,CAACiC,GAAG,CAAID,MAAM,CAACE,IAAI,CAACC,SAA+B,EAAEH,MAAM,CAACI,IAAI,CAACC,QAAQ,CAAC;EAC1F;EAuDA;EACO,OAAOJ,GAAGA,CAAA,EAAe;IAAA,SAAAK,KAAA,GAAAlC,SAAA,CAAAC,MAAA,EAAXkC,IAAW,OAAAhC,KAAA,CAAA+B,KAAA,GAAAE,KAAA,MAAAA,KAAA,GAAAF,KAAA,EAAAE,KAAA;MAAXD,IAAW,CAAAC,KAAA,IAAApC,SAAA,CAAAoC,KAAA;IAAA;IAC5B,OAAO,IAAIxC,KAAK,CAAC,GAAGJ,kCAAkC,CAACP,gBAAgB,CAACkD,IAAI,CAAC,CAAC,CAAC;EACnF;EA+BA,IAAWrC,MAAMA,CAAA;IAAK,OAAO,IAAI,CAACW,OAAO;EAAE;EAC3C,IAAWR,MAAMA,CAAA;IAAK,OAAO,IAAI,CAACoC,OAAO;EAAE;EAC3C,IAAW/B,MAAMA,CAAA;IAAK,OAAO,IAAI,CAACI,OAAO;EAAE;EAC3C,IAAW4B,OAAOA,CAAA;IAAK,OAAO,IAAI,CAACC,YAAY;EAAE;EAE1CC,KAAKA,CAAA,EAAsB;IAAA,IAArBlC,MAAM,GAAAN,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAY,SAAA,GAAAZ,SAAA,MAAG,IAAI,CAACU,OAAO;IAC9B,OAAO,IAAId,KAAK,CAAI,IAAI,CAACa,OAAO,EAAEH,MAAM,CAAC;EAC7C;EAEOmC,SAASA,CAAoBC,IAAO;IACvC,OAAO,IAAI,CAACC,WAAW,CAAC,IAAI,CAACC,cAAc,CAACF,IAAI,CAAC,CAAiB;EACtE;EACOC,WAAWA,CAA2BE,KAAa;IACtD,OAAO,IAAI,CAACC,UAAU,CAACD,KAAK,CAAC;EACjC;EACOD,cAAcA,CAAoBF,IAAO;IAC5C,OAAO,IAAI,CAACjC,OAAO,CAACD,MAAM,CAACuC,SAAS,CAAEC,CAAC,IAAKA,CAAC,CAACN,IAAI,KAAKA,IAAI,CAAC;EAChE;EACOI,UAAUA,CAA2BD,KAAa;IACrD,IAAIA,KAAK,GAAG,CAAC,IAAIA,KAAK,IAAI,IAAI,CAACI,WAAW,EAAE;MAAE,OAAO,IAAI;;IACzD,IAAIC,KAAe,EAAEC,KAAgB;IACrC,MAAM3C,MAAM,GAAI,IAAI,CAACC,OAAuB,CAACD,MAAM;IACnD,MAAM4C,OAAO,GAAG,IAAI,CAACC,SAAS,KAAK,IAAI,CAACA,SAAS,GAAG,EAAE,CAAa;IACnE,IAAIF,KAAK,GAAGC,OAAO,CAACP,KAAK,CAAC,EAAE;MAAE,OAAOM,KAAkB;;IACvD,IAAID,KAAK,GAAG1C,MAAM,CAACqC,KAAK,CAAC,EAAE;MACvB,MAAMvC,MAAM,GAAG,IAAI,CAACI,OAAO,CACtB4C,GAAG,CAAEC,KAAK,IAAKA,KAAK,CAACT,UAAU,CAAID,KAAK,CAAC,CAAC,CAC1CW,MAAM,CAAEC,GAAG,IAAuBA,GAAG,IAAI,IAAI,CAAC;MACnD,IAAInD,MAAM,CAACL,MAAM,GAAG,CAAC,EAAE;QACnB,OAAQmD,OAAO,CAACP,KAAK,CAAC,GAAG,IAAIlE,MAAM,CAAIuE,KAAK,EAAE5C,MAAM,CAAC;;;IAG7D,OAAO,IAAI;EACf;EAEA;EACOoD,SAASA,CAAA,EAAmC;IAAA,IAAlCC,QAAQ,GAAA3D,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAY,SAAA,GAAAZ,SAAA,MAAG,QAAQ;IAAA,IAAE4D,MAAM,GAAA5D,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAY,SAAA,GAAAZ,SAAA,MAAG,IAAI;IAC/C,MAAM6D,MAAM,GAAG,CAACD,MAAM,GAChBtE,qBAAqB,GACrBC,uBAAuB;IAC7B,OAAOsE,MAAM,CAACC,QAAQ,CAAC,IAAI,CAAC,CAACC,YAAY,CAAC,IAAI,CAAC;EACnD;EACOC,KAAKA,CAAA;IACR,OAAO,IAAI,CAAC3B,OAAO;EACvB;EACO4B,MAAMA,CAAA,EAA6C;IACtD,MAAMC,WAAW,GAAG,IAAI,CAACzD,OAAO,CAACD,MAAM,CAAC2D,MAAM,CAAC,CAACC,CAAC,EAAEpB,CAAC,EAAEqB,CAAC,KAAKD,CAAC,CAACE,GAAG,CAACtB,CAAC,CAACN,IAAS,EAAE2B,CAAC,CAAC,EAAE,IAAIE,GAAG,EAAa,CAAC;IAAC,SAAAC,KAAA,GAAAxE,SAAA,CAAAC,MAAA,EADnEwE,WAAgB,OAAAtE,KAAA,CAAAqE,KAAA,GAAAE,KAAA,MAAAA,KAAA,GAAAF,KAAA,EAAAE,KAAA;MAAhBD,WAAgB,CAAAC,KAAA,IAAA1E,SAAA,CAAA0E,KAAA;IAAA;IAEtD,OAAO,IAAI,CAACC,QAAQ,CAAC,GAAGF,WAAW,CAACnB,GAAG,CAAEsB,UAAU,IAAKV,WAAW,CAACW,GAAG,CAACD,UAAU,CAAE,CAAC,CAACpB,MAAM,CAAEsB,CAAC,IAAKA,CAAC,GAAG,CAAC,CAAC,CAAC,CAAC;EAChH;EACOH,QAAQA,CAAA,EAAuD;IAAA,SAAAI,KAAA,GAAA/E,SAAA,CAAAC,MAAA,EAAvB+E,aAAuB,OAAA7E,KAAA,CAAA4E,KAAA,GAAAE,KAAA,MAAAA,KAAA,GAAAF,KAAA,EAAAE,KAAA;MAAvBD,aAAuB,CAAAC,KAAA,IAAAjF,SAAA,CAAAiF,KAAA;IAAA;IAClE,MAAMnF,MAAM,GAAG,IAAI,CAACW,OAAO,CAACkE,QAAQ,CAAI,GAAGK,aAAa,CAAC;IACzD,OAAO,IAAIpF,KAAK,CAACE,MAAM,EAAE,IAAI,CAACY,OAAO,CAAC4C,GAAG,CAAC4B,IAAA,IAAoC;MAAA,IAAnC;QAAEjF,MAAM;QAAE6B,IAAI,EAAE;UAAEC;QAAS;MAAE,CAAE,GAAAmD,IAAA;MACtE,OAAO,IAAIrG,WAAW,CAACiB,MAAM,EAAEG,MAAM,EAAE+E,aAAa,CAAC1B,GAAG,CAAEe,CAAC,IAAKtC,SAAS,CAACsC,CAAC,CAAC,CAAC,CAACb,MAAM,CAAC2B,OAAO,CAAC,CAAC;IAClG,CAAC,CAAC,CAAC;EACP;EACOC,MAAMA,CAA8CC,KAAe;IAAA,IAAAC,KAAA;IAEtE,MAAM9E,MAAM,GAAG,IAAI,CAACC,OAAO,CAACD,MAAM;IAClC,MAAM,CAAC+E,OAAO,EAAEC,QAAQ,CAAC,GAAGH,KAAK,CAACvF,MAAM,CAACU,MAAM,CAAC2D,MAAM,CAAC,CAACsB,IAAI,EAAEC,EAAE,EAAEC,MAAM,KAAI;MACxE,MAAM,CAACJ,OAAO,EAAEC,QAAQ,CAAC,GAAGC,IAAI;MAChC,MAAMpB,CAAC,GAAG7D,MAAM,CAACuC,SAAS,CAAEC,CAAC,IAAKA,CAAC,CAACN,IAAI,KAAKgD,EAAE,CAAChD,IAAI,CAAC;MACrD,CAAC2B,CAAC,GAAImB,QAAQ,CAACnB,CAAC,CAAC,GAAGsB,MAAM,GAAIJ,OAAO,CAAC/D,IAAI,CAACmE,MAAM,CAAC;MAClD,OAAOF,IAAI;IACf,CAAC,EAAE,CAAC,EAAE,EAAE,EAAE,CAAe,CAAC;IAE1B,MAAM3F,MAAM,GAAG,IAAI,CAACW,OAAO,CAAC2E,MAAM,CAACC,KAAK,CAACvF,MAAM,CAAC;IAChD,MAAMsD,OAAO,GAAG,CACZ,GAAG5C,MAAM,CAAC8C,GAAG,CAAC,UAACsC,EAAE,EAAEvB,CAAC,EAAEwB,GAAG;MAAA,IAAEC,CAAC,GAAA9F,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAY,SAAA,GAAAZ,SAAA,MAAGwF,QAAQ,CAACnB,CAAC,CAAC;MAAA,OACrCyB,CAAC,KAAKlF,SAAS,GAAG0E,KAAI,CAAC3C,WAAW,CAAC0B,CAAC,CAAC,GAAGgB,KAAK,CAAC1C,WAAW,CAACmD,CAAC,CAAC;IAAA,CAAE,CAAC,EACpE,GAAGP,OAAO,CAACjC,GAAG,CAAEe,CAAC,IAAKgB,KAAK,CAAC1C,WAAW,CAAC0B,CAAC,CAAE,CAAC,CAC/C,CAACb,MAAM,CAAC2B,OAAO,CAAyC;IAEzD,OAAO,IAAIvF,KAAK,CAAQ,GAAGH,kCAAkC,CAAMK,MAAM,EAAEsD,OAAO,CAAC,CAAC;EACxF;;AAGJ,SAASpC,iBAAiBA,CAA2DF,KAA6C;EAC9H,MAAM;IAAEkB;EAAI,CAAE,GAAGlB,KAAK;EACtB,IAAIkB,IAAI,YAAYhD,MAAM,EAAE;IACxB,OAAOY,KAAK,CAAC+B,UAAU,CAAChC,YAAY,CAACkB,IAAI,CAACC,KAA+C,CAAC,CAAC;;EAE/F,OAAO,IAAI;AACf;AAEA,SAASG,sBAAsBA,CAA2DH,KAAkD;EACxI,MAAM;IAAEkB;EAAI,CAAE,GAAGlB,KAAK;EACtB,IAAIkB,IAAI,YAAYhD,MAAM,EAAE;IACxB,OAAOW,YAAY,CAACkB,IAAI,CAACC,KAAoD,CAAC,CAACiF,IAAI,CAAEnE,MAAM,IAAKhC,KAAK,CAAC+B,UAAU,CAACC,MAAM,CAAC,CAAC;;EAE7H,OAAO,IAAI;AACf","ignoreList":[]},"metadata":{},"sourceType":"module"}