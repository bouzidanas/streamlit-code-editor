{"ast":null,"code":"// Licensed to the Apache Software Foundation (ASF) under one\n// or more contributor license agreements.  See the NOTICE file\n// distributed with this work for additional information\n// regarding copyright ownership.  The ASF licenses this file\n// to you under the Apache License, Version 2.0 (the\n// \"License\"); you may not use this file except in compliance\n// with the License.  You may obtain a copy of the License at\n//\n//   http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing,\n// software distributed under the License is distributed on an\n// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n// KIND, either express or implied.  See the License for the\n// specific language governing permissions and limitations\n// under the License.\nimport { Data } from '../data';\nimport { Schema } from '../schema';\nimport { Chunked } from '../vector/chunked';\nimport { RecordBatch } from '../recordbatch';\nconst noopBuf = new Uint8Array(0);\nconst nullBufs = bitmapLength => [noopBuf, noopBuf, new Uint8Array(bitmapLength), noopBuf];\n/** @ignore */\nexport function ensureSameLengthData(schema, chunks) {\n  let batchLength = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : chunks.reduce((l, c) => Math.max(l, c.length), 0);\n  let data;\n  let field;\n  let i = -1,\n    n = chunks.length;\n  const fields = [...schema.fields];\n  const batchData = [];\n  const bitmapLength = (batchLength + 63 & ~63) >> 3;\n  while (++i < n) {\n    if ((data = chunks[i]) && data.length === batchLength) {\n      batchData[i] = data;\n    } else {\n      (field = fields[i]).nullable || (fields[i] = fields[i].clone({\n        nullable: true\n      }));\n      batchData[i] = data ? data._changeLengthAndBackfillNullBitmap(batchLength) : Data.new(field.type, 0, batchLength, batchLength, nullBufs(bitmapLength));\n    }\n  }\n  return [new Schema(fields), batchLength, batchData];\n}\n/** @ignore */\nexport function distributeColumnsIntoRecordBatches(columns) {\n  return distributeVectorsIntoRecordBatches(new Schema(columns.map(_ref => {\n    let {\n      field\n    } = _ref;\n    return field;\n  })), columns);\n}\n/** @ignore */\nexport function distributeVectorsIntoRecordBatches(schema, vecs) {\n  return uniformlyDistributeChunksAcrossRecordBatches(schema, vecs.map(v => v instanceof Chunked ? v.chunks.map(c => c.data) : [v.data]));\n}\n/** @ignore */\nfunction uniformlyDistributeChunksAcrossRecordBatches(schema, columns) {\n  const fields = [...schema.fields];\n  const batchArgs = [];\n  const memo = {\n    numBatches: columns.reduce((n, c) => Math.max(n, c.length), 0)\n  };\n  let numBatches = 0,\n    batchLength = 0;\n  let i = -1,\n    numColumns = columns.length;\n  let child,\n    childData = [];\n  while (memo.numBatches-- > 0) {\n    for (batchLength = Number.POSITIVE_INFINITY, i = -1; ++i < numColumns;) {\n      childData[i] = child = columns[i].shift();\n      batchLength = Math.min(batchLength, child ? child.length : batchLength);\n    }\n    if (isFinite(batchLength)) {\n      childData = distributeChildData(fields, batchLength, childData, columns, memo);\n      if (batchLength > 0) {\n        batchArgs[numBatches++] = [batchLength, childData.slice()];\n      }\n    }\n  }\n  return [schema = new Schema(fields, schema.metadata), batchArgs.map(xs => new RecordBatch(schema, ...xs))];\n}\n/** @ignore */\nfunction distributeChildData(fields, batchLength, childData, columns, memo) {\n  let data;\n  let field;\n  let length = 0,\n    i = -1,\n    n = columns.length;\n  const bitmapLength = (batchLength + 63 & ~63) >> 3;\n  while (++i < n) {\n    if ((data = childData[i]) && (length = data.length) >= batchLength) {\n      if (length === batchLength) {\n        childData[i] = data;\n      } else {\n        childData[i] = data.slice(0, batchLength);\n        data = data.slice(batchLength, length - batchLength);\n        memo.numBatches = Math.max(memo.numBatches, columns[i].unshift(data));\n      }\n    } else {\n      (field = fields[i]).nullable || (fields[i] = field.clone({\n        nullable: true\n      }));\n      childData[i] = data ? data._changeLengthAndBackfillNullBitmap(batchLength) : Data.new(field.type, 0, batchLength, batchLength, nullBufs(bitmapLength));\n    }\n  }\n  return childData;\n}","map":{"version":3,"mappings":"AAAA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAKA,SAASA,IAAI,QAAiB,SAAS;AACvC,SAASC,MAAM,QAAe,WAAW;AACzC,SAASC,OAAO,QAAQ,mBAAmB;AAC3C,SAASC,WAAW,QAAQ,gBAAgB;AAE5C,MAAMC,OAAO,GAAG,IAAIC,UAAU,CAAC,CAAC,CAAC;AACjC,MAAMC,QAAQ,GAAIC,YAAoB,IAAe,CACjDH,OAAO,EAAEA,OAAO,EAAE,IAAIC,UAAU,CAACE,YAAY,CAAC,EAAEH,OAAO,CAC1C;AAEjB;AACA,OAAM,SAAUI,oBAAoB,CAChCC,MAAiB,EACjBC,MAA0B,EACqC;EAAA,IAA/DC,WAAW,uEAAGD,MAAM,CAACE,MAAM,CAAC,CAACC,CAAC,EAAEC,CAAC,KAAKC,IAAI,CAACC,GAAG,CAACH,CAAC,EAAEC,CAAC,CAACG,MAAM,CAAC,EAAE,CAAC,CAAC;EAE/D,IAAIC,IAAsB;EAC1B,IAAIC,KAAwB;EAC5B,IAAIC,CAAC,GAAG,CAAC,CAAC;IAAEC,CAAC,GAAGX,MAAM,CAACO,MAAM;EAC7B,MAAMK,MAAM,GAAG,CAAC,GAAGb,MAAM,CAACa,MAAM,CAAC;EACjC,MAAMC,SAAS,GAAG,EAAwB;EAC1C,MAAMhB,YAAY,GAAG,CAAEI,WAAW,GAAG,EAAE,GAAI,CAAC,EAAE,KAAK,CAAC;EACpD,OAAO,EAAES,CAAC,GAAGC,CAAC,EAAE;IACZ,IAAI,CAACH,IAAI,GAAGR,MAAM,CAACU,CAAC,CAAC,KAAKF,IAAI,CAACD,MAAM,KAAKN,WAAW,EAAE;MACnDY,SAAS,CAACH,CAAC,CAAC,GAAGF,IAAI;KACtB,MAAM;MACH,CAACC,KAAK,GAAGG,MAAM,CAACF,CAAC,CAAC,EAAEI,QAAQ,KAAKF,MAAM,CAACF,CAAC,CAAC,GAAGE,MAAM,CAACF,CAAC,CAAC,CAACK,KAAK,CAAC;QAAED,QAAQ,EAAE;MAAI,CAAE,CAAsB,CAAC;MACtGD,SAAS,CAACH,CAAC,CAAC,GAAGF,IAAI,GAAGA,IAAI,CAACQ,kCAAkC,CAACf,WAAW,CAAC,GACpEX,IAAI,CAAC2B,GAAG,CAACR,KAAK,CAACS,IAAI,EAAE,CAAC,EAAEjB,WAAW,EAAEA,WAAW,EAAEL,QAAQ,CAACC,YAAY,CAAC,CAAqB;;;EAG3G,OAAO,CAAC,IAAIN,MAAM,CAAIqB,MAAM,CAAC,EAAEX,WAAW,EAAEY,SAAS,CAA4C;AACrG;AAEA;AACA,OAAM,SAAUM,kCAAkC,CAA8CC,OAA6B;EACzH,OAAOC,kCAAkC,CAAI,IAAI9B,MAAM,CAAI6B,OAAO,CAACE,GAAG,CAAC;IAAA,IAAC;MAAEb;IAAK,CAAE;IAAA,OAAKA,KAAK;EAAA,EAAC,CAAC,EAAEW,OAAO,CAAC;AAC3G;AAEA;AACA,OAAM,SAAUC,kCAAkC,CAA8CtB,MAAiB,EAAEwB,IAAkD;EACjK,OAAOC,4CAA4C,CAAIzB,MAAM,EAAEwB,IAAI,CAACD,GAAG,CAAEG,CAAC,IAAKA,CAAC,YAAYjC,OAAO,GAAGiC,CAAC,CAACzB,MAAM,CAACsB,GAAG,CAAElB,CAAC,IAAKA,CAAC,CAACI,IAAI,CAAC,GAAG,CAACiB,CAAC,CAACjB,IAAI,CAAC,CAAC,CAAC;AAClJ;AAEA;AACA,SAASgB,4CAA4C,CAA8CzB,MAAiB,EAAEqB,OAA6B;EAE/I,MAAMR,MAAM,GAAG,CAAC,GAAGb,MAAM,CAACa,MAAM,CAAC;EACjC,MAAMc,SAAS,GAAG,EAAoC;EACtD,MAAMC,IAAI,GAAG;IAAEC,UAAU,EAAER,OAAO,CAAClB,MAAM,CAAC,CAACS,CAAC,EAAEP,CAAC,KAAKC,IAAI,CAACC,GAAG,CAACK,CAAC,EAAEP,CAAC,CAACG,MAAM,CAAC,EAAE,CAAC;EAAC,CAAE;EAE/E,IAAIqB,UAAU,GAAG,CAAC;IAAE3B,WAAW,GAAG,CAAC;EACnC,IAAIS,CAAC,GAAW,CAAC,CAAC;IAAEmB,UAAU,GAAGT,OAAO,CAACb,MAAM;EAC/C,IAAIuB,KAAuB;IAAEC,SAAS,GAAuB,EAAE;EAE/D,OAAOJ,IAAI,CAACC,UAAU,EAAE,GAAG,CAAC,EAAE;IAE1B,KAAK3B,WAAW,GAAG+B,MAAM,CAACC,iBAAiB,EAAEvB,CAAC,GAAG,CAAC,CAAC,EAAE,EAAEA,CAAC,GAAGmB,UAAU,GAAG;MACpEE,SAAS,CAACrB,CAAC,CAAC,GAAGoB,KAAK,GAAGV,OAAO,CAACV,CAAC,CAAC,CAACwB,KAAK,EAAG;MAC1CjC,WAAW,GAAGI,IAAI,CAAC8B,GAAG,CAAClC,WAAW,EAAE6B,KAAK,GAAGA,KAAK,CAACvB,MAAM,GAAGN,WAAW,CAAC;;IAG3E,IAAImC,QAAQ,CAACnC,WAAW,CAAC,EAAE;MACvB8B,SAAS,GAAGM,mBAAmB,CAACzB,MAAM,EAAEX,WAAW,EAAE8B,SAAS,EAAEX,OAAO,EAAEO,IAAI,CAAC;MAC9E,IAAI1B,WAAW,GAAG,CAAC,EAAE;QACjByB,SAAS,CAACE,UAAU,EAAE,CAAC,GAAG,CAAC3B,WAAW,EAAE8B,SAAS,CAACO,KAAK,EAAE,CAAC;;;;EAItE,OAAO,CACHvC,MAAM,GAAG,IAAIR,MAAM,CAAIqB,MAAM,EAAEb,MAAM,CAACwC,QAAQ,CAAC,EAC/Cb,SAAS,CAACJ,GAAG,CAAEkB,EAAE,IAAK,IAAI/C,WAAW,CAACM,MAAM,EAAE,GAAGyC,EAAE,CAAC,CAAC,CACxD;AACL;AAEA;AACA,SAASH,mBAAmB,CAA8CzB,MAA2B,EAAEX,WAAmB,EAAE8B,SAA6B,EAAEX,OAA6B,EAAEO,IAA4B;EAClN,IAAInB,IAAsB;EAC1B,IAAIC,KAAwB;EAC5B,IAAIF,MAAM,GAAG,CAAC;IAAEG,CAAC,GAAG,CAAC,CAAC;IAAEC,CAAC,GAAGS,OAAO,CAACb,MAAM;EAC1C,MAAMV,YAAY,GAAG,CAAEI,WAAW,GAAG,EAAE,GAAI,CAAC,EAAE,KAAK,CAAC;EACpD,OAAO,EAAES,CAAC,GAAGC,CAAC,EAAE;IACZ,IAAI,CAACH,IAAI,GAAGuB,SAAS,CAACrB,CAAC,CAAC,KAAM,CAACH,MAAM,GAAGC,IAAI,CAACD,MAAM,KAAKN,WAAY,EAAE;MAClE,IAAIM,MAAM,KAAKN,WAAW,EAAE;QACxB8B,SAAS,CAACrB,CAAC,CAAC,GAAGF,IAAI;OACtB,MAAM;QACHuB,SAAS,CAACrB,CAAC,CAAC,GAAGF,IAAI,CAAC8B,KAAK,CAAC,CAAC,EAAErC,WAAW,CAAC;QACzCO,IAAI,GAAGA,IAAI,CAAC8B,KAAK,CAACrC,WAAW,EAAEM,MAAM,GAAGN,WAAW,CAAC;QACpD0B,IAAI,CAACC,UAAU,GAAGvB,IAAI,CAACC,GAAG,CAACqB,IAAI,CAACC,UAAU,EAAER,OAAO,CAACV,CAAC,CAAC,CAAC+B,OAAO,CAACjC,IAAI,CAAC,CAAC;;KAE5E,MAAM;MACH,CAACC,KAAK,GAAGG,MAAM,CAACF,CAAC,CAAC,EAAEI,QAAQ,KAAKF,MAAM,CAACF,CAAC,CAAC,GAAGD,KAAK,CAACM,KAAK,CAAC;QAAED,QAAQ,EAAE;MAAI,CAAE,CAAsB,CAAC;MAClGiB,SAAS,CAACrB,CAAC,CAAC,GAAGF,IAAI,GAAGA,IAAI,CAACQ,kCAAkC,CAACf,WAAW,CAAC,GACpEX,IAAI,CAAC2B,GAAG,CAACR,KAAK,CAACS,IAAI,EAAE,CAAC,EAAEjB,WAAW,EAAEA,WAAW,EAAEL,QAAQ,CAACC,YAAY,CAAC,CAAqB;;;EAG3G,OAAOkC,SAAS;AACpB","names":["Data","Schema","Chunked","RecordBatch","noopBuf","Uint8Array","nullBufs","bitmapLength","ensureSameLengthData","schema","chunks","batchLength","reduce","l","c","Math","max","length","data","field","i","n","fields","batchData","nullable","clone","_changeLengthAndBackfillNullBitmap","new","type","distributeColumnsIntoRecordBatches","columns","distributeVectorsIntoRecordBatches","map","vecs","uniformlyDistributeChunksAcrossRecordBatches","v","batchArgs","memo","numBatches","numColumns","child","childData","Number","POSITIVE_INFINITY","shift","min","isFinite","distributeChildData","slice","metadata","xs","unshift"],"sources":["util/recordbatch.ts"],"sourcesContent":["// Licensed to the Apache Software Foundation (ASF) under one\n// or more contributor license agreements.  See the NOTICE file\n// distributed with this work for additional information\n// regarding copyright ownership.  The ASF licenses this file\n// to you under the Apache License, Version 2.0 (the\n// \"License\"); you may not use this file except in compliance\n// with the License.  You may obtain a copy of the License at\n//\n//   http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing,\n// software distributed under the License is distributed on an\n// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n// KIND, either express or implied.  See the License for the\n// specific language governing permissions and limitations\n// under the License.\n\nimport { Column } from '../column';\nimport { Vector } from '../vector';\nimport { DataType } from '../type';\nimport { Data, Buffers } from '../data';\nimport { Schema, Field } from '../schema';\nimport { Chunked } from '../vector/chunked';\nimport { RecordBatch } from '../recordbatch';\n\nconst noopBuf = new Uint8Array(0);\nconst nullBufs = (bitmapLength: number) => <unknown> [\n    noopBuf, noopBuf, new Uint8Array(bitmapLength), noopBuf\n] as Buffers<any>;\n\n/** @ignore */\nexport function ensureSameLengthData<T extends { [key: string]: DataType } = any>(\n    schema: Schema<T>,\n    chunks: Data<T[keyof T]>[],\n    batchLength = chunks.reduce((l, c) => Math.max(l, c.length), 0)\n) {\n    let data: Data<T[keyof T]>;\n    let field: Field<T[keyof T]>;\n    let i = -1, n = chunks.length;\n    const fields = [...schema.fields];\n    const batchData = [] as Data<T[keyof T]>[];\n    const bitmapLength = ((batchLength + 63) & ~63) >> 3;\n    while (++i < n) {\n        if ((data = chunks[i]) && data.length === batchLength) {\n            batchData[i] = data;\n        } else {\n            (field = fields[i]).nullable || (fields[i] = fields[i].clone({ nullable: true }) as Field<T[keyof T]>);\n            batchData[i] = data ? data._changeLengthAndBackfillNullBitmap(batchLength)\n                : Data.new(field.type, 0, batchLength, batchLength, nullBufs(bitmapLength)) as Data<T[keyof T]>;\n        }\n    }\n    return [new Schema<T>(fields), batchLength, batchData] as [Schema<T>, number, Data<T[keyof T]>[]];\n}\n\n/** @ignore */\nexport function distributeColumnsIntoRecordBatches<T extends { [key: string]: DataType } = any>(columns: Column<T[keyof T]>[]): [Schema<T>, RecordBatch<T>[]] {\n    return distributeVectorsIntoRecordBatches<T>(new Schema<T>(columns.map(({ field }) => field)), columns);\n}\n\n/** @ignore */\nexport function distributeVectorsIntoRecordBatches<T extends { [key: string]: DataType } = any>(schema: Schema<T>, vecs: (Vector<T[keyof T]> | Chunked<T[keyof T]>)[]): [Schema<T>, RecordBatch<T>[]] {\n    return uniformlyDistributeChunksAcrossRecordBatches<T>(schema, vecs.map((v) => v instanceof Chunked ? v.chunks.map((c) => c.data) : [v.data]));\n}\n\n/** @ignore */\nfunction uniformlyDistributeChunksAcrossRecordBatches<T extends { [key: string]: DataType } = any>(schema: Schema<T>, columns: Data<T[keyof T]>[][]): [Schema<T>, RecordBatch<T>[]] {\n\n    const fields = [...schema.fields];\n    const batchArgs = [] as [number, Data<T[keyof T]>[]][];\n    const memo = { numBatches: columns.reduce((n, c) => Math.max(n, c.length), 0) };\n\n    let numBatches = 0, batchLength = 0;\n    let i: number = -1, numColumns = columns.length;\n    let child: Data<T[keyof T]>, childData: Data<T[keyof T]>[] = [];\n\n    while (memo.numBatches-- > 0) {\n\n        for (batchLength = Number.POSITIVE_INFINITY, i = -1; ++i < numColumns;) {\n            childData[i] = child = columns[i].shift()!;\n            batchLength = Math.min(batchLength, child ? child.length : batchLength);\n        }\n\n        if (isFinite(batchLength)) {\n            childData = distributeChildData(fields, batchLength, childData, columns, memo);\n            if (batchLength > 0) {\n                batchArgs[numBatches++] = [batchLength, childData.slice()];\n            }\n        }\n    }\n    return [\n        schema = new Schema<T>(fields, schema.metadata),\n        batchArgs.map((xs) => new RecordBatch(schema, ...xs))\n    ];\n}\n\n/** @ignore */\nfunction distributeChildData<T extends { [key: string]: DataType } = any>(fields: Field<T[keyof T]>[], batchLength: number, childData: Data<T[keyof T]>[], columns: Data<T[keyof T]>[][], memo: { numBatches: number }) {\n    let data: Data<T[keyof T]>;\n    let field: Field<T[keyof T]>;\n    let length = 0, i = -1, n = columns.length;\n    const bitmapLength = ((batchLength + 63) & ~63) >> 3;\n    while (++i < n) {\n        if ((data = childData[i]) && ((length = data.length) >= batchLength)) {\n            if (length === batchLength) {\n                childData[i] = data;\n            } else {\n                childData[i] = data.slice(0, batchLength);\n                data = data.slice(batchLength, length - batchLength);\n                memo.numBatches = Math.max(memo.numBatches, columns[i].unshift(data));\n            }\n        } else {\n            (field = fields[i]).nullable || (fields[i] = field.clone({ nullable: true }) as Field<T[keyof T]>);\n            childData[i] = data ? data._changeLengthAndBackfillNullBitmap(batchLength)\n                : Data.new(field.type, 0, batchLength, batchLength, nullBufs(bitmapLength)) as Data<T[keyof T]>;\n        }\n    }\n    return childData;\n}\n"]},"metadata":{},"sourceType":"module"}