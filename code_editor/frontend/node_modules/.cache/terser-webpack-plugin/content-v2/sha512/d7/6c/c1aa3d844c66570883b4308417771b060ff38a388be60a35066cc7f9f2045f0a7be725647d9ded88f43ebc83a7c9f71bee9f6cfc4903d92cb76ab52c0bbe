{"code":"ace.define(\"ace/ext/simple_tokenizer\",[\"require\",\"exports\",\"module\",\"ace/tokenizer\",\"ace/layer/text_util\"],(function(e,t,n){\"use strict\";var i=e(\"../tokenizer\").Tokenizer,o=e(\"../layer/text_util\").isTextToken,r=function(){function e(e,t){this._lines=e.split(/\\r\\n|\\r|\\n/),this._states=[],this._tokenizer=t}return e.prototype.getTokens=function(e){var t=this._lines[e],n=this._states[e-1],i=this._tokenizer.getLineTokens(t,n);return this._states[e]=i.state,i.tokens},e.prototype.getLength=function(){return this._lines.length},e}();n.exports={tokenize:function(e,t){for(var n=new r(e,new i(t.getRules())),s=[],u=0;u<n.getLength();u++){var a=n.getTokens(u);s.push(a.map((function(e){return{className:o(e.type)?void 0:\"ace_\"+e.type.replace(/\\./g,\" ace_\"),value:e.value}})))}return s}}})),ace.require([\"ace/ext/simple_tokenizer\"],(function(e){\"object\"==typeof module&&\"object\"==typeof exports&&module&&(module.exports=e)}));","name":"1b732133ea32ebe0aa4979f7113993e1.js","input":"ace.define(\"ace/ext/simple_tokenizer\", [\"require\", \"exports\", \"module\", \"ace/tokenizer\", \"ace/layer/text_util\"], function (require, exports, module) {\n  \"use strict\";\n\n  var Tokenizer = require(\"../tokenizer\").Tokenizer;\n  var isTextToken = require(\"../layer/text_util\").isTextToken;\n  var SimpleTokenizer = /** @class */function () {\n    function SimpleTokenizer(content, tokenizer) {\n      this._lines = content.split(/\\r\\n|\\r|\\n/);\n      this._states = [];\n      this._tokenizer = tokenizer;\n    }\n    SimpleTokenizer.prototype.getTokens = function (row) {\n      var line = this._lines[row];\n      var previousState = this._states[row - 1];\n      var data = this._tokenizer.getLineTokens(line, previousState);\n      this._states[row] = data.state;\n      return data.tokens;\n    };\n    SimpleTokenizer.prototype.getLength = function () {\n      return this._lines.length;\n    };\n    return SimpleTokenizer;\n  }();\n  function tokenize(content, highlightRules) {\n    var tokenizer = new SimpleTokenizer(content, new Tokenizer(highlightRules.getRules()));\n    var result = [];\n    for (var lineIndex = 0; lineIndex < tokenizer.getLength(); lineIndex++) {\n      var lineTokens = tokenizer.getTokens(lineIndex);\n      result.push(lineTokens.map(function (token) {\n        return {\n          className: isTextToken(token.type) ? undefined : \"ace_\" + token.type.replace(/\\./g, \" ace_\"),\n          value: token.value\n        };\n      }));\n    }\n    return result;\n  }\n  module.exports = {\n    tokenize: tokenize\n  };\n});\n(function () {\n  ace.require([\"ace/ext/simple_tokenizer\"], function (m) {\n    if (typeof module == \"object\" && typeof exports == \"object\" && module) {\n      module.exports = m;\n    }\n  });\n})();"}